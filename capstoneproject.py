# -*- coding: utf-8 -*-
"""capstoneproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11xdRdLXnKOEaeZX1PWHAoJHsoR6rn2pg
"""

!pip install apache-beam
!pip install great_expectations
!pip install apache-airflow
!pip install apache-flink
!pip install great_expectations==0.15.49
!great_expectations init
!pip install pandas
!pip install apache-beam
!pip install great_expectations
!pip install apache-airflow
!pip install apache-flink
!pip install great_expectations==0.15.49
!great_expectations init

import requests
import json
import apache_beam as beam
import psycopg2
import great_expectations as ge
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import logging

# --- Configurations ---
APP_ID = "YOUR_APP_ID"
API_KEY = "YOUR_API_KEY"
PRIMARY_DB_CONN_STR = "YOUR_AZURE_POSTGRESQL_CONN_STR"
SECONDARY_DB_CONN_STR = "YOUR_LOCAL_POSTGRESQL_CONN_STR"
GE_CONTEXT_ROOT_DIR = "./great_expectations"
STATION_IDS = ["STATION_ID1", "STATION_ID2", "STATION_ID3"]
EXPECTATION_SUITE_NAME = "train_timetable_expectations"

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

"""**2. Data Ingestion
2.1. Real-Time Data (Apache Flink)**
"""

from pyflink.datastream import StreamExecutionEnvironment
from pyflink.table import StreamTableEnvironment

# Initialize Flink environment
env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)

# Define a source table for real-time data
t_env.execute_sql("""
    CREATE TABLE train_departures (
        station STRING,
        train_uid STRING,
        aimed_departure_time TIMESTAMP,
        status STRING
    ) WITH (
        'connector' = 'kafka',
        'topic' = 'train_departures',
        'properties.bootstrap.servers' = 'localhost:9092',
        'format' = 'json'
    )
""")

"""**. Scheduled Data (Python requests)**"""

def get_train_data(station_id):
    url = f"https://transportapi.com/v3/uk/train/station/{station_id}/live.json?app_id={APP_ID}&app_key={API_KEY}&from_offset=0&to_offset=30&live=true"
    try:
        response = requests.get(url)
        response.raise_for_status()
        return response.json()
    except requests.RequestException as e:
        logger.error(f"API request failed for {station_id}: {e}")
        return None

"""**3. Data Validation (Great Expectations)**"""

# Initialize Great Expectations context
context = ge.DataContext(GE_CONTEXT_ROOT_DIR)

def validate_data_with_ge(data):
    if not data:
        return None
    batch = context.get_batch(
        batch_kwargs={"dataset": data},
        expectation_suite_name=EXPECTATION_SUITE_NAME
    )
    results = context.run_validation_operator("action_list_operator", assets_to_validate=[batch])
    if not results.success:
        logger.warning("Data validation failed!")
        return None
    return data

"""**4. Data Transformation (Apache Beam)**"""

def transform_data(data):
    if not data:
        return None
    transformed = []
    for train in data.get("departures", {}).get("all", []):
        transformed.append({
            "station": data.get("station_name"),
            "train_uid": train.get("train_uid"),
            "aimed_departure_time": train.get("aimed_departure_time"),
            "expected_departure_time": train.get("expected_departure_time"),
            "status": train.get("status"),
        })
    return transformed

"""**5. Data Storage (PostgreSQL)**"""

def load_to_postgres(data, conn_str):
    if not data:
        return
    try:
        conn = psycopg2.connect(conn_str)
        cursor = conn.cursor()
        for train in data.get("departures", {}).get("all", []):
            cursor.execute(
                """
                INSERT INTO train_timetable (station, train_uid, aimed_departure_time, status)
                VALUES (%s, %s, %s, %s)
                ON CONFLICT (train_uid) DO UPDATE SET status = EXCLUDED.status
                """,
                (data.get("station_name"), train.get("train_uid"), train.get("aimed_departure_time"), train.get("status"))
            )
        conn.commit()
        cursor.close()
        conn.close()
    except Exception as e:
        logger.error(f"Error inserting data into PostgreSQL: {e}")

"""**6. Apache Beam Pipeline**"""

def run_beam_pipeline():
    with beam.Pipeline() as pipeline:
        (
            pipeline
            | "Create Station IDs" >> beam.Create(STATION_IDS)
            | "Fetch Train Data" >> beam.Map(get_train_data)
            | "Validate Data" >> beam.Map(validate_data_with_ge)
            | "Transform Data" >> beam.Map(transform_data)
            | "Load to Primary DB" >> beam.Map(lambda data: load_to_postgres(data, PRIMARY_DB_CONN_STR))
            | "Load to Backup DB" >> beam.Map(lambda data: load_to_postgres(data, SECONDARY_DB_CONN_STR))
        )

"""**7. Airflow DAG Orchestration**"""

# Airflow DAG configuration
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2024, 2, 1),
    "retries": 2,
    "retry_delay": timedelta(minutes=10),
}

# Define the DAG
with DAG(
    dag_id="train_data_pipeline",
    schedule_interval="@hourly",
    default_args=default_args,
    catchup=False,
) as dag:
    ingest_data_task = PythonOperator(
        task_id="ingest_data",
        python_callable=run_beam_pipeline,
    )

"""**8. Monitoring & Alerting**"""

from airflow.utils.email import send_email

def notify_failure(context):
    subject = f"Airflow Task Failed: {context.get('task_instance').task_id}"
    body = f"Task failed with exception: {context.get('exception')}"
    send_email(to="your-email@example.com", subject=subject, html_content=body)

# Add failure callback to default_args
default_args["on_failure_callback"] = notify_failure

"""**9. Main Execution**"""

import apache_beam as beam

def run_beam_pipeline():
    with beam.Pipeline() as pipeline:
        (
            pipeline
            | "Create Station IDs" >> beam.Create(STATION_IDS)
            | "Fetch Train Data" >> beam.Map(get_train_data)
            | "Validate Data" >> beam.Map(validate_data_with_ge)
            | "Transform Data" >> beam.Map(transform_data)
            | "Load to Primary DB" >> beam.Map(lambda data: load_to_postgres(data, PRIMARY_DB_CONN_STR))
            | "Load to Backup DB" >> beam.Map(lambda data: load_to_postgres(data, SECONDARY_DB_CONN_STR))
        )